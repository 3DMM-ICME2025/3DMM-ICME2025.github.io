<!DOCTYPE html>
<html lang="en">

<head>
    <title>3DMM</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

 
    <link rel="icon" type="image/x-ico" href="imgs/logo.png" />
    

    <link href="https://fonts.googleapis.com/css?family=B612+Mono|Cabin:400,700&display=swap" rel="stylesheet">

    <link rel="stylesheet" href="fonts/icomoon/style.css">
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css"
          integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">

    <link rel="stylesheet" href="css/jquery-ui.css">
    <link rel="stylesheet" href="css/owl.carousel.min.css">
    <link rel="stylesheet" href="css/owl.theme.default.min.css">
    <link rel="stylesheet" href="css/owl.theme.default.min.css">

    <link rel="stylesheet" href="css/jquery.fancybox.min.css">

    <link rel="stylesheet" href="fonts/flaticon/font/flaticon.css">

    <link rel="stylesheet" href="css/aos.css">
    <link href="css/jquery.mb.YTPlayer.min.css" media="all" rel="stylesheet" type="text/css">

    <link rel="stylesheet" href="css/style.css">



    <script>
        (function (i, s, o, g, r, a, m) {
            i['GoogleAnalyticsObject'] = r;
            i[r] = i[r] || function () {
                (i[r].q = i[r].q || []).push(arguments)
            }, i[r].l = 1 * new Date();
            a = s.createElement(o),
                m = s.getElementsByTagName(o)[0];
            a.async = 1;
            a.src = g;
            m.parentNode.insertBefore(a, m)
        })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');

        ga('create', 'UA-88572407-1', 'auto');
        ga('send', 'pageview');
    </script>
</head>

<body data-spy="scroll" data-target=".site-navbar-target" data-offset="300">



<div class="site-wrap">

    <div class="site-mobile-menu site-navbar-target">
        <div class="site-mobile-menu-header">

            <div class="site-mobile-menu-close mt-3">
                <span class="icon-close2 js-menu-toggle"></span>
            </div>
        </div>
        <div class="site-mobile-menu-body"></div>
    </div>

<!---------------------------导航栏------------------------------->
    <div class="site-navbar py-2 js-sticky-header site-navbar-target d-none pl-0 d-lg-block" role="banner" > 


            <div class="d-flex align-items-center">
             

                <div class="mr-auto">

               
                    <nav class="site-navigation position-relative text-right" role="navigation">
                     <a href="http://iccv2021.thecvf.com/"> <img class='iccv' src="imgs/icme.png" width=190px height=47px style="margin-top:-1px;"></a>

               

                        <ul class="site-menu main-menu js-clone-nav mr-auto d-none pl-0 d-lg-block">
                            <li>
                           
                                <a href="index.html"><img src="imgs/logo.png" width=190px height=47px style="margin-left: 200px; margin-top: -3.5px; position: sticky;"></a>
                            </li>
                            <li>
                                
                                <a href="index.html" class="nav-link text-left" style="font-size:15px; font-family:Georgia, serif;">Home</a>
                            </li>
                            
                            <li>
                                <a href="index.html#call for papers" class="nav-link text-left" style="font-size:15px; font-family:Georgia, serif">Call for papers</a>
                            </li>

                            <li>
                                <a href="index.html#dates" class="nav-link text-left" style="font-size:15px; font-family:Georgia, serif">important dates</a>
                            </li>

                            <li>
                                <a href="index.html#invited speakers" class="nav-link text-left" style="font-size:15px; font-family:Georgia, serif">invited speakers</a>
                            </li>



                            <li>
                                <a href="index.html#organizers" class="nav-link text-left" style="font-size:15px; font-family:Georgia, serif">organizers</a>
                            </li>


                            </li>
                        </ul>
                    </nav>

                </div>

            </div>
        </div>

    </div>

</div>

<div class="site-blocks-cover overlay inner-page-cover" style="background-image: url('imgs/background.png');" 


     data-stellar-background-ratio="0.5">
    <div class="container">
        <div class="row align-items-center justify-content-center">
            <div class="col-md-10 text-center" data-aos="fade-up">

                <br>
            
                <h1 style="font-size:46px">3D Multimedia</h1>
                <h1 style="font-size:46px">Analytics, Search and Generation</h1>

                <br><br><br>
                <h4> In Conjunction with ICME 2025</h4>
                <h4> June 30 - July 4, Nantes, France</h4>
            </div>
        </div>
    </div>
</div>



<div class="site-section">
    <div class="container">

<!------------------------------ news  ------------------------------------------>


<div class="col-lg-12" id="news" style="padding-top:80px;margin-top:-150px;">
        <h4><i>News</i> !</h4>
        <ul>           
            <li>
                <p style="height: 10px">
                    <strong style="font-size:20px;color:red;font-family:'Times New Roman';"> Jan 10, 2025:&nbsp;&thinsp;&thinsp;</strong>The website is coming. Call for papers.
                </p>
            </li>                             
        </ul>
</div>
<br><br><br><br>

<!------------------------  overview  ----------------------->
            <div class="col-lg-12">
                <div class="section-title">
                    <h2>Overview</h2>
                    <br>
                    <h4 style="font-size: 21px;"><i></i></h4>
                </div>
                <div class="trend-entry d-flex">
                    <div class="trend-contents">
                        <p>

                            &emsp;&emsp;  Today, ubiquitous multimedia sensors and large-scale computing infrastructures are producing at a rapid velocity of <font face='Times New Roman'>3D</font> multi-modality data, such as <font face='Times New Roman'>3D</font> point cloud acquired with LIDAR sensors, RGB-D videos recorded by Kinect cameras, meshes of varying topology, and volumetric data. <font face='Times New Roman'>3D</font> multimedia combines different content forms such as text, audio, images, and video with <font face='Times New Roman'>3D</font> information, which can perceive the world better since the real world is 3-dimensional instead of 2-dimensional. For example, the robots can manipulate objects successfully by recognizing the object via RGB frames and perceiving the object size via point cloud. Researchers have strived to push the limits of <font face='Times New Roman'>3D</font> multimedia search and generation in various applications, such as autonomous driving, robotic visual navigation, smart industrial manufacturing, logistics distribution, and logistics picking. The <font face='Times New Roman'>3D</font> multimedia (e.g., the videos and point cloud) can also help the agents to grasp, move and place the packages automatically in logistics picking systems.
Therefore, <font face='Times New Roman'>3D</font> multimedia analytics is one of the fundamental problems in multimedia understanding. Different from <font face='Times New Roman'>3D</font> vision, <font face='Times New Roman'>3D</font> multimedia analytics mainly concentrate on fusing the <font face='Times New Roman'>3D</font> content with other media. It is a very challenging problem that involves multiple tasks such as human <font face='Times New Roman'>3D</font> mesh recovery and analysis, <font face='Times New Roman'>3D</font> shapes and scenes generation from real-world data, <font face='Times New Roman'>3D</font> virtual talking head, <font face='Times New Roman'>3D</font> multimedia classification and retrieval, <font face='Times New Roman'>3D</font> semantic segmentation, <font face='Times New Roman'>3D</font> object detection and tracking, <font face='Times New Roman'>3D</font> multimedia scene understanding, and so on. Therefore, the purpose of this workshop is to: 1) bring together the state-of-the-art research on <font face='Times New Roman'>3D</font> multimedia analysis; 2) call for a coordinated effort to understand the opportunities and challenges emerging in <font face='Times New Roman'>3D</font> multimedia analysis; 3) identify key tasks and evaluate the state-of-the-art methods; 4) showcase innovative methodologies and ideas; 5) introduce interesting real-world <font face='Times New Roman'>3D</font> multimedia analysis systems or applications; and 6) propose new real-world or simulated datasets and discuss future directions. We solicit original contributions in all fields of <font face='Times New Roman'>3D</font> multimedia analysis that explore the multi-modality data to generate the strong <font face='Times New Roman'>3D</font> data representation. We believe this workshop will offer a timely collection of research updates to benefit researchers and practitioners in the broad multimedia communities.
                            <br>
                        </p>
                    </div>
                </div>
            </div>
<!------------------------  Call for papers  --------------------->
<div class="col-lg-12" id="call for papers" style="padding-top:80px;margin-top:-80px;">
        <div class="section-title">

            <br><br><br>
            <h2>Call for papers</h2>
        </div>

        <div class="trend-entry d-flex">
                <div class="trend-contents">
                    <p style="margin: auto;">
                        &emsp; &emsp;We invite submissions for ICME <font face='Times New Roman'>2025</font> Workshop, <font face='Times New Roman'>3D</font> Multimedia Analytics, Search and Generation (<font face="Times New Roman">3DMM2025</font>), which brings researchers together to discuss robust, interpretable, and responsible technologies for <font face='Times New Roman'>3D</font> multimedia analysis. We solicit original research and survey papers that must be no longer than <font face="Times New Roman" size=5px>6</font> pages (including all text, figures, and references). Each submitted paper will be peer-reviewed by at least three reviewers. All accepted papers will be presented as either oral or poster presentations, with the best paper award. Papers that violate anonymity, do not use the ICME submission template will be rejected without review. By submitting a manuscript to this workshop, the authors acknowledge that no paper substantially similar in content has been submitted to another workshop or conference during the review period. Authors should prepare their manuscript according to the Guide for Authors of ICME. For detailed instructions, see <a href="https://2025.ieeeicme.org/author-information-and-submission-instructions/" style="font-family: 'Times New Roman', Times, serif; font-size:21px;">here</a>. Submission address is <a href="https://cmt3.research.microsoft.com/ICMEW2025/" style="font-family: 'Times New Roman', Times, serif; font-size:21px;">here</a>.
                        <br>
                        &emsp;&emsp;The scope of this workshop includes, but is not limited to, the following topics:
                    </p>
                <ul class="set_ul" style="margin-bottom: 0px; margin-left:20px; display: inline-block;">
                        <li>
                            Generative Models for <font face='Times New Roman'>3D</font> Multimedia and <font face='Times New Roman'>3D</font> Multimedia Synthesis
                        </li>
                        <li>
                            Generating <font face='Times New Roman'>3D</font> Multimedia from Real-world Data
                        </li>
                        <li>
                            <font face='Times New Roman'>3D</font> Multimodal Analysis and Description
                        </li>
                        <li>
                            Multimedia Virtual/Augmented Reality
                        </li>
                        <li>
                            <font face='Times New Roman'>3D</font> Multimedia Systems
                        </li>
                        
                        <li>
                            <font face='Times New Roman'>3D</font> Multimedia Search and Recommendation
                        </li>
                       
                        <li>
                            Mobile <font face='Times New Roman'>3D</font> Multimedia
                        </li>
                        <li>
                            <font face='Times New Roman'>3D</font> Shape Estimation and Reconstruction
                        </li>
                        <li>
                            <font face='Times New Roman'>3D</font> Scene and Object Understanding
                        </li>                        
                        
                        <li>
                            High-level Representation of <font face='Times New Roman'>3D</font> Multimedia Data
                        </li>
                        <li>
                            <font face='Times New Roman'>3D</font> Multimedia Application in Industry
                        </li>                                              
                    </ul> 
                    <p style="margin: auto;">
                        &emsp;&emsp;<strong>Fast Review for Rejected Regular Submissions of ICME 2025</strong>
                        <br>
                        &emsp;&emsp;We set up a Fast Review mechanism for the regular submissions rejected by the ICME main conference. We strongly encourage the rejected papers to be submitted to this workshop. In order to submit through Fast Review, authors must write a front letter (1 page) to clarify the revision of the paper and attach all previous reviews. All the papers submitted through Fast Review will be directly reviewed by meta-reviewers to make the decisions.
                    </p>                                      
                </div>
            </div>
    </div>

    



<!------------------------------ Invited speakers  ------------------------------------------>

<div class="col-lg-12" id="invited speakers" style="padding-top:80px;margin-top:-80px;">
    <div class="section-title">

        <br><br><br>
        <h2>Invited speakers</h2>
        <br><br>
     
        <div align="left">
            <div class="instructor_fina" style="float: left; display: inline; margin-top: 1px;">
                <a href="https://faculty.bjtu.edu.cn/8497/">
                    <div class="instructorphoto"><img src="imgs/JinYi.png"></div>
                </a>
                <div style="font-family:Helvetica, sans-serif; font-size: 17px;display: inline-block;">Prof.</div>
                <a href="https://faculty.bjtu.edu.cn/8497/">
                    <div style="font-size: 17px;display: inline-block;">&thinsp;Yi Jin</div>
                </a>
                <div style="font-family:Helvetica, sans-serif; font-size: 17px">Beijing Jiaotong University, China</div>
            </div>  

            <div style="font-family:Times New Roman, sans-serif; font-size: 20px; display: inline-block; width: 78%;text-align: justify;">
                <strong>Title:</strong> Research on Key Techniques of Task-Oriented Point Cloud Sampling Based on Deep Learning
               <br>
                <strong>Abstract:</strong> With the rapid advancement of 3D perception technologies and exponential growth of data scales, sampling techniques have become fundamental preprocessing operations in computer vision. As a critical component of 3D data processing, point cloud sampling not only serves the dual purposes of dimensionality reduction and quality enhancement, but more importantly determines the performance of subsequent perception tasks. This presentation systematically elaborates recent advances in integrating task-oriented sampling with deep learning. Following the proposed "point cloud representation - adaptive sampling mechanism - task performance optimization" framework, this work innovatively establishes a bio-inspired task-oriented sampling theory: Through developing multi-level deep network architectures, key technical bottlenecks have been overcome in inter-point relationship modeling, local feature extraction, regional similarity discrimination, and cross-region correlation mining, achieving remarkable improvements in both sampling quality and task generalization. Finally, the discussion will focus on how multimodal fusion technologies are reshaping the design paradigms and industrial landscapes of next-generation intelligent systems.
               <br>
               <strong>Biography:</strong> Yi Jin Professor is currently serves as Assistant Dean at the School of Computer and Information Technology, Beijing Jiaotong University. Yi Jin received a Ph.D. from Beijing Jiaotong University, China, in 2010. Her research interests include semantic understanding of traffic video, image processing, and computer vision. Yi Jin has been honored with multiple prestigious awards, including the Nomination for the IEEE Computer Society Annual Best Paper Award (2022), Silver Medal at the 50th International Exhibition of Inventions Geneva (2025), Gold Medal of the Macao International Innovation and Invention Expo (2024), Second Prize of the Innovation Achievement Award from the China Industry-University-Research Institute Collaboration Association (2023), Second Prize of the Science and Technology Award from the China Institute of Communications (2024). Professor Yi Jin has published over 70 high-quality papers in top-tier journals and conferences, including IEEE TIFS, IEEE TMM, AAAI, and ACM MM. Among these, five are ESI Highly Cited Papers. She has served as the editor-in-chief or co-editor of three academic books and textbooks, and contributed to three national and industry standards. She has led or participated in over 20 research projects at the national and ministerial levels, while securing 1 international patent and 35 Chinese invention patents. Yi Jin serves on the editorial boards of two prominent SCI-indexed journals: Journal of The Franklin Instituteand Journal of Electronic Imaging. She also contributes as a guest editor for several leading international and domestic journals in her field.
            </div>	

        </div>
         <!------------------------------ the above is jin yi ------------------------------------------>
        <br>
        <br>
        <br>
        <br> 

        <!------------------------------ The second speaker info ------------------------------------------>
        <div align="left">
            <div class="instructor_fina" style="float: left; display: inline; margin-top: 1px;">
                <a href="https://wbhu.github.io/">
                    <div class="instructorphoto"><img src="imgs/HuWenbo.png"></div>
                </a>
                <div style="font-family:Helvetica, sans-serif; font-size: 17px;display: inline-block;">Dr.</div>
                <a href="https://wbhu.github.io/">
                    <div style="font-size: 17px;display: inline-block;">&thinsp;Wenbo Hu</div>
                </a>
                <div style="font-family:Helvetica, sans-serif; font-size: 17px">Tencent ARC Lab, China</div>
            </div>  

            <div style="font-family:Times New Roman, sans-serif; font-size: 20px; display: inline-block; width: 78%;text-align: justify;">
                <strong>Title:</strong> GenConstruction: The Mutual Benefit between Content Generation and Reconstruction
               <br>
                <strong>Abstract:</strong> Video generation models have demonstrated remarkable content generation capabilities and hold great potential as world simulators. However, due to their fundamental nature of modeling in a 2D space, challenges persist in ensuring 3D rationality and consistency. Meanwhile, 3D foundation models have made significant progress, yet the recovery of accurate and stable 3D information from 2D observations in open-world scenarios remains a formidable task. This study discovers that content generation and reconstruction can mutually benefit each other, facilitating a spiral upward development. Specifically, leveraging the capabilities of generation models can significantly enhance the accuracy and stability of 3D reconstruction for 3D foundation models. Conversely, the structural information provided by 3D foundation models can greatly improve the 3D consistency and controllability of video generation models. In this talk, we will share a series of our exploratory works in this direction and discuss the evolving paths of generation and reconstruction.
               <br>
               <strong>Biography:</strong> Wenbo Hu is currently a senior researcher at Tencent ARC Lab, leading an effort to Generative World Model, including 3D from Images/Videos, Novel View Synthesis, and Video Generation. He obtained his Ph.D. degree in Computer Science and Engineering from The Chinese University of Hong Kong (CUHK) in 2022. His work has been selected as Best Paper Finalist in ICCV'2023. He received the CCF Elite Collegiate Award in 2017. He has served as a reviewer for top-tier conferences and journals, including SIGGRAPH, SIGGRAPH Asia, CVPR, ICCV, ECCV, NeurIPS, ICML, EG, TVCG, IJCV, etc.
            </div>
        </div>
        <!------------------------------ The second speaker info END------------------------------------------>
        
        <br>
        <br>
        <br>
        <br> 
        <!------------------------------ The 3th speaker info ------------------------------------------>
        <div align="left">
            <div class="instructor_fina" style="float: left; display: inline; margin-top: 1px;">
                <a href="https://person.zju.edu.cn/0095152">
                    <div class="instructorphoto"><img src="imgs/WeiDong.png"></div>
                </a>
                <div style="font-family:Helvetica, sans-serif; font-size: 17px;display: inline-block;">Prof.</div>
                <a href="https://person.zju.edu.cn/0095152">
                    <div style="font-size: 17px;display: inline-block;">&thinsp;Weidong Geng</div>
                </a>
                <div style="font-family:Helvetica, sans-serif; font-size: 17px">Zhejiang University, China</div>
            </div>  

            <div style="font-family:Times New Roman, sans-serif; font-size: 20px; display: inline-block; width: 78%;text-align: justify;">
                <strong>Title:</strong> Multi-modal Modelling of Body language for Digital Human
                <br>
                <strong>Abstract:</strong> HumanAIGC is the cross-discipline research area of Digital Human and AIGC. This talk will present how to model body language to drive digital human with correlated visual-audio modals, especial on how to choreograph,stylize and personalize gestures and postures for digital human with textual description,  speech and music. The application of AIGC-based Digital Human for TV-program making will also be discussed.
                <br>
                <strong>Biography:</strong> WEIDONG GENG, Professor, College of Computer Science & Technology, Zhejiang University, PR China. From 1995 to 2000, he was in Zhejiang University, where his research interests are in CAD/CG, and intelligent systems. He joined Fraunhofer Institute for Media Communication (former GMD.IMK), Germany, as a research scientist in 2000. In 2002, he worked in Multimedia Innovation Center, The Hong Kong Polytechnic University, Hong Kong. Since 2003, he has been working in State Key Laboratory of CAD&CG, Zhejiang University, and his current research focuses on computer aided design, digital human, perceptual user interface, interactive media and AIGC. 
                
            </div>
        </div>
        <!------------------------------ The 3th speaker info END------------------------------------------>

</div>
</div>

<!------------------------------ Invited speakers  END ------------------------------------------>





<!----------------------------  organizers  ---------------------------------------->
<div class="col-lg-12" id="organizers" style="padding-top:80px;margin-top:-80px;">

    <!----------------------------  organizer  ---------------------------------------->
        <div class="section-title">
            <br><br><br>
            <h2>Organizers</h2>
        </div>
    
        <div align="center">

                <!--------------  dai peng --------->
                <div class="instructor_mine">
                    <a href="https://pdaicode.github.io/">
                        <div class="instructorphoto"><img src="imgs/DaiPeng1.jpg"></div>
                        <div style="font-size: 18px">Peng Dai</div>
                    </a>
                    <div style="font-family:Helvetica, sans-serif;font-size: 18px">Noah’s Ark Lab, Canada</div>
                </div>

                <!--------------  an shan  --------->
                <div class="instructor_mine">
                    <a href="https://anshan.ai/">
                        <div class="instructorphoto"><img src="imgs/AnShan.jpg"></div>
                        <div style="font-size: 18px">Shan An</div>
                    </a>
                    <div style="font-family:Helvetica, sans-serif;font-size: 18px">JD Health, China</div>
                </div>
              
                <!--------------  liukun  --------->
                <div class="instructor">
                    <a href="https://scholar.google.com/citations?user=NMMB7wcAAAAJ&hl=en">
                        <div class="instructorphoto">
                                <img src="imgs/YiboHu.JPG">
                        </div>
                        <div style="font-size: 18px">Kun Liu</div>
                    </a>
                    <div style="font-family:Helvetica, sans-serif;font-size: 18px">Explore Academy of JD.com, China</div>
                </div>         
                

                <!--------------  Ge Xu Ri --------->
                <div class="instructor">
                    <a href="https://xurige1995.github.io/">
                        <div class="instructorphoto">
                                <img src="imgs/gexuri.png">
                        </div>
                        <div style="font-size: 18px">Xuri Ge</div>
                    </a>
                    <div style="font-family:Helvetica, sans-serif;font-size: 18px">University of Glasgow, UK</div>
                </div>

               <!--------------  Guoxin Wang   --------->
                <div class="instructor">
                        <div class="instructorphoto">
                                <img src="imgs/WangGuoXin.jpg">
                        </div>
                        <div style="font-size: 18px">Guoxin Wang</div>

                    <div style="font-family:Helvetica, sans-serif;font-size: 18px">Zhejiang University, China</div>
                </div>

                <!--------------    liuwu  --------->
                <div class="instructor_mine">
                    <a href="https://drliuwu.com/">
                        <div class="instructorphoto"><img src="imgs/JinggenLiu.png"></div>
                        <div style="font-size: 18px">Wu Liu</div>
                    </a>
                    <div style="font-family:Helvetica, sans-serif;font-size: 18px">University of Science and Technology of China, China</div>
                </div>
                
        
                <!--------------  8 Antonios Gasteratos   --------->
                <div class="instructor_mine">
                    <a href="https://robotics.pme.duth.gr/antonis/">
                        <div class="instructorphoto"><img src="imgs/Antonios.jpg"></div>
                        <div style="font-size: 18px">Antonios Gasteratos</div>
                    </a>
                    <div style="font-family:Helvetica, sans-serif;font-size: 18px">Democritus University of Thrace, Greece</div>
                </div>
        </div>

        <!----------------------------  Committe Chairs  ---------------------------------------->
       
 
        
    </div>
      

<!------------------------  Workshop Agendas  --------------------->

<div class="col-lg-12" id="agendas" style="padding-top:80px;margin-top:-80px;">
    <div class="section-title">
        <br><br><br>
        <h2>Workshop Agenda</h2>
    </div>
    <div class="trend-entry d-flex">
        <table class="table table-striped" style="border-bottom:2px solid #C4C4C4;border-top:2px solid #C4C4C4;width:70%; line-height: 22px;" align="center">
            <thead>
            <!-- <tr style="background-color:#BFEFFF;"> -->
            <tr>
                <th scope="col" style="text-align: center;" width="100">Date</th>
                <th scope="col" style="text-align: center;" width="500"> Description</th>
            </tr>
            </thead>
            <tbody>
            <!-- <tr style="background-color:#8BB1D8;"> -->
            <tr style="background-color:#B6CEE7;">    
                <td >9:30-9:40 </td>
                <td >Opening</td>
            </tr>
            <tr>
                <td >9:40-10:05</td>
                <td>Keynote 1: Research on Key Techniques of Task-Oriented Point Cloud Sampling Based on Deep Learning</td>
            </tr>
            <!--  <td height="25px">9:20-10:00 </td>   class="AutoNewline" -->
            <tr>
                <td >10:05-10:30 </td>                
                <td >Keynote 2: Cross-Modal Vision-and-Language Intelligence: Methodologies and Applications</td>
            </tr>
            <tr>
                <td>10:30-10:55</td>
                <td>Keynote 3: Muldi-modal Modelling of Body Language for Digital Human</td>
            </tr>
            <tr>
                <td >10:55-12:15</td>
                <td >8 Oral Presentation (~10min * 8)</td>
            </tr>   
            <tr>
                <td>12:15-12:20</td>
                <td> Announce the Best Paper Award, Discussion and Closing</td>
            </tr>            
            </tbody>
            
        </table> 
    </div>
</div>

<tr>
    <td></td>
    <td></td>
</tr>




<!------------------------  Oral Orders  --------------------->
<div class="col-lg-12" id="Oral" style="padding-top:80px;margin-top:-80px;">
    <div class="section-title">
        <br><br><br>
        <h2>Accepted Papers</h2>
    </div>
    <div class="trend-entry d-flex">
        <table class="table table-striped" style="border-bottom:2px solid #C4C4C4;border-top:2px solid #C4C4C4;width:70%; line-height: 18px;" align="center">
            <thead>
            <!-- <tr style="background-color:#BFEFFF;"> -->
            <tr>
                <th scope="col" style="text-align: center;" width="10%">Oral Order</th> 
                <th scope="col" style="text-align: center;" width="15%">Date</th> 
                <th scope="col" style="text-align: center;" width="75%"> Paper Title</th>
            </tr>
            </thead>
            <tbody>
            <!-- <tr style="background-color:#8BB1D8;"> -->
            <tr style="background-color:#B6CEE7;">    
                <td>1</td>
                <td>10:55-11:05</td>
                <td>Keypoint Ensemble For Image Matching</td>
            </tr>
            <tr>
                <td>2</td>
                <td>11:05-11:15</td>
                <td>MF-Adapter: Better 3D Foundation Model with Multimodal Fusion Adapter</td>
            </tr>
            <tr>
                <td>3</td>
                <td>11:15-11:25</td>
                <td>DHGS: Decoupled Hybrid Gaussian Splatting for Driving Scene</td>
            </tr>
            <tr>
                <td>4</td>
                <td>11:25-11:35</td>
                <td>Optimizing Cooperative Multi-Object Tracking using Graph Signal Processing</td>
            </tr>
            <tr>
                <td>5</td>
                <td>11:35-11:45</td>
                <td>Guided Model-based LiDAR Super-Resolution for Resource-Efficient Automotive scene Segmentation</td>
            </tr>           
            <tr>
                <td>6</td>
                <td>11:45-11:55</td>
                <td>LIVE-FIT: LED-based Immersive Virtual Environment with Fusion, Interaction, and Transmission</td>
            </tr>  
            <tr>
                <td>7</td>
                <td>11:55-12:05</td>
                <td>Benchmarking Learnable Mesh and Texture Representations for Immersive Digital Twins</td>
            </tr>         
            <tr>
                <td>8</td>
                <td>12:05-12:15</td>
                <td>MVLLaVA: An Intelligent Agent for Unified and Flexible Novel View Synthesis</td>
            </tr>                  
            </tbody>            
        </table> 
    </div>
</div>
<!------------------------  Oral Orders END --------------------->





<!------------------------------email-------------------------------->
<br><br>
<p style="margin-top:30px;margin-bottom: 60px; text-align: center; font-family: 'Times New Roman', Times, serif; font-size: 24px;">Previous Workshops on 3DMM: <a href="https://3DMM-ICME2022.github.io/" style="font-family: 'Times New Roman', Times, serif; font-size:21px;">3DMM-ICME2022,</a><a href="https://3DMM-ICME2023.github.io/" style="font-family: 'Times New Roman', Times, serif; font-size:21px;"> 3DMM-ICME2023</a> <a href="https://3DMM-ICME2024.github.io/" style="font-family: 'Times New Roman', Times, serif; font-size:21px;">3DMM-ICME2024</a> </p>
<p style="margin-top:30px;margin-bottom: 60px; text-align: center; font-family: 'Times New Roman', Times, serif; font-size: 24px;">If you have any questions, feel free to contact &lt peng [DOT] dai [DOT] ca [AT] ieee.org</p>

<!-------------------------------   boarder  ---------------------------------------->
        <div class="col-lg-12">
            <div style="display:inline-block;width:500px;">
                <script type="text/javascript" src="//rc.rev
            olvermaps.com/0/0/7.js?i=2hlmeh3dic1&amp;m=0&amp;c=ff0000&amp;cr1=ffffff&amp;br=19&amp;sx=0"
                        async="async"></script>
            </div>
        </div>
    </div>
</div>
</div>
<!-- END section -->


<div class="footer">
    <div class="container">
        <div class="row">
            <div class="col-12">
                <div class="copyright">
                    <p>
                        <!-- Link back to Colorlib can't be removed. Template is licensed under CC BY 3.0. -->
                        Copyright &copy;<script>document.write(new Date().getFullYear());</script>
                        All rights reserved | Powered by <a
                            href="https://colorlib.com" target="_blank">Colorlib</a>
                        <!-- Link back to Colorlib can't be removed. Template is licensed under CC BY 3.0. -->
                    </p>
                </div>
            </div>
        </div>
    </div>
</div>


</div>
<!-- .site-wrap -->


<!-- loader -->
<div id="loader" class="show fullscreen">
    <svg class="circular" width="48px" height="48px">
        <circle class="path-bg" cx="24" cy="24" r="22" fill="none" stroke-width="4" stroke="#eeeeee"/>
        <circle class="path" cx="24" cy="24" r="22" fill="none" stroke-width="4" stroke-miterlimit="10"
                stroke="#ff5e15"/>
    </svg>
</div>

<script src="js/jquery-3.3.1.min.js"></script>
<script src="js/jquery-migrate-3.0.1.min.js"></script>
<script src="js/jquery-ui.js"></script>
<script src="js/popper.min.js"></script>
<script src="js/bootstrap.min.js"></script>
<script src="js/owl.carousel.min.js"></script>
<script src="js/jquery.stellar.min.js"></script>
<script src="js/jquery.countdown.min.js"></script>
<script src="js/bootstrap-datepicker.min.js"></script>
<script src="js/jquery.easing.1.3.js"></script>
<script src="js/aos.js"></script>
<script src="js/jquery.fancybox.min.js"></script>
<script src="js/jquery.sticky.js"></script>
<script src="js/jquery.mb.YTPlayer.min.js"></script>


<script src="js/main.js"></script>

</body>

</html>
